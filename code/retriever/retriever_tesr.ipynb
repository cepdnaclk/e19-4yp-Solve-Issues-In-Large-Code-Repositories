{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lahiru-menik/miniconda3/envs/agentless/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.agents import tool\n",
    "\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "load_dotenv()\n",
    "import commit\n",
    "\n",
    "def neighbors_by_relation(G, node, relation_type):\n",
    "    \n",
    "    neighbors = []\n",
    "    for u, v, data in G.edges(node, data=True):\n",
    "        if data.get('relation') == relation_type:\n",
    "            neighbor = v if u == node else u  # Handle undirected edges\n",
    "            neighbors.append(neighbor)\n",
    "    return neighbors\n",
    "\n",
    "def load_graph(pickle_path):\n",
    "    \"\"\"Loads a NetworkX DiGraph from a pickle file.\"\"\"\n",
    "    with open(pickle_path, \"rb\") as f:\n",
    "        graph = pickle.load(f)\n",
    "    return graph\n",
    "\n",
    "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")\n",
    "# filter = [0, 1, 2, 3, 4,5, 6, 7, 8, 9]\n",
    "# dataset = dataset.select(filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function_classes(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_content = file.read()\n",
    "            parsed_data = ast.parse(file_content)\n",
    "    except Exception as e:  # Catch all types of exceptions\n",
    "        print(f\"Error in file {file_path}: {e}\")\n",
    "        return [], [], \"\"\n",
    "    info = []\n",
    "\n",
    "    for node in ast.walk(parsed_data):\n",
    "        if isinstance(node, ast.ClassDef):\n",
    "            info.append(node.name)\n",
    "           \n",
    "        elif isinstance(node, ast.FunctionDef) or isinstance(\n",
    "            node, ast.AsyncFunctionDef\n",
    "        ):\n",
    "            if node.name ==\"__init__\":\n",
    "                continue\n",
    "            info.append(node.name)             \n",
    "    return info\n",
    "\n",
    "import os\n",
    "\n",
    "def get_file_structure(root_dir: str) -> dict:\n",
    "    file_structure = {}\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "      paths = dirpath.split(\"/\")\n",
    "      \n",
    "      filenames = [file for file in filenames if file.endswith('.py')]\n",
    "      rel_path = os.path.join(root_dir, dirpath)\n",
    "      rel_path = \".\" if rel_path == \".\" else rel_path.replace(\"\\\\\", \"/\")\n",
    "      if \"test\" in dirpath:\n",
    "          continue\n",
    "      if not filenames:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "      filenames = [dirpath+\"/\"+file for file in filenames]\n",
    "      file_structure[dirpath] = filenames\n",
    "    return file_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prompts\n",
    "import schema\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    #max_retries=2,\n",
    ")\n",
    "\n",
    "llm_deepseek = ChatDeepSeek(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "llm_large = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_retries=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1661582/485085973.py:4: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputFunctionsParser()\n",
    "\n",
    "model_extract = llm_large.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "extract_chain = prompts.prompt_extract | model_extract\n",
    "\n",
    "model_select = llm.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.FileSuspicionOutput)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "select_chain = prompts.file_path_filter_prompt | model_select\n",
    "\n",
    "model_filter_list = llm.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "filter_list_chain = prompts.get_suspicious_file_list_from_list_of_files_prompt | model_filter_list \n",
    "\n",
    "model_select_list = llm.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "select_list_chain = prompts.suspicious_files_filter_list_usingclfn_prompt | model_select_list\n",
    "\n",
    "model_select_with_reason = llm.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFileReasoningOutput)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "\n",
    "select_with_reason_chain = prompts.suspicious_files_with_reason_prompt | model_select_with_reason\n",
    "\n",
    "model_select_directory = llm.bind(\n",
    "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousDirectoryOutput)],\n",
    "    function_call=\"auto\",\n",
    ")\n",
    "select_directory_chain = prompts.suspicious_directory_prompt | model_select_directory\n",
    "\n",
    "deep_reasoning_chain = prompts.deep_reasoning_prompt | llm_deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "commit_id = dataset[i]['base_commit']\n",
    "name = dataset[i]['instance_id'].split(\"__\")[0]\n",
    "problem_description = dataset[i]['problem_statement']\n",
    "graph = load_graph(f\"graph_{name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 751\n",
      "\tPrompt Tokens: 718\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 33\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.002125\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as callback:\n",
    "    result = extract_chain.invoke({\"problem_description\": problem_description})\n",
    "    # print(result)\n",
    "    print(callback)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'arguments': '{\"file\":\"django/views/debug.py\",\"class_function_name\":\"SafeExceptionReporterFilter.get_safe_settings\"}', 'name': 'SuspiciousComponentOutput'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 718, 'total_tokens': 751, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_90122d973c', 'id': 'chatcmpl-BVm26VRoIxjAE2fqfW8rH7GpqHb0V', 'finish_reason': 'function_call', 'logprobs': None} id='run-e5cce633-5578-484e-97fd-73ba6e64972b-0' usage_metadata={'input_tokens': 718, 'output_tokens': 33, 'total_tokens': 751, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from commit import update\n",
    "def start(inputs):\n",
    "    problem_description = inputs['problem_description']\n",
    "    name = inputs['name']\n",
    "    graph = inputs['graph'] \n",
    "    commit_id = inputs['commit_id']\n",
    "    graph = inputs['graph'] \n",
    "    update(name, commit_id)\n",
    "    with get_openai_callback() as callback:\n",
    "        result = extract_chain.invoke({\"problem_description\": problem_description})\n",
    "        print(callback)\n",
    "    result = result.additional_kwargs['function_call']['arguments']\n",
    "    result = json.loads(result)\n",
    "    result['name'] = name\n",
    "    result['problem_description'] = problem_description\n",
    "    result['graph'] = graph\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(start(problem_description, name,graph, commit_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.compress import get_skeleton\n",
    "import json\n",
    "def get_most_suspicious_files(inputs):\n",
    "    \"\"\"\n",
    "    Given a graph and a file, find the most suspicious files related to the given file.\n",
    "    \"\"\"\n",
    "    problem_description = inputs['problem_description']\n",
    "    graph = inputs['graph']\n",
    "    file = inputs['file']\n",
    "    if \"/\" in file:\n",
    "        file = file.split(\"/\")[-1]\n",
    "    if \".\" in file:\n",
    "        file = file.split(\".\")[0]\n",
    "    suspicious_files = []\n",
    "    for neighbor in neighbors_by_relation(graph, \"module_\"+file,  'path'):\n",
    "        if \"test\" in neighbor:\n",
    "            continue\n",
    "        suspicious_files.append(neighbor)\n",
    "    with get_openai_callback() as callback:  \n",
    "        filtered = select_chain.invoke({\"problem_description\": problem_description, \"file_list\": suspicious_files})\n",
    "        print(callback)\n",
    "    filtered = json.loads(filtered.additional_kwargs['function_call']['arguments'])\n",
    "    selected_file = filtered['suspicious_file']\n",
    "    candiate_structure = {}\n",
    "    for neighbor in neighbors_by_relation(graph, selected_file,  'imports')+[selected_file]:\n",
    "        try:\n",
    "            # with open(neighbor, \"r\", encoding=\"utf-8\") as f:\n",
    "            #     raw_code = f.read()\n",
    "            candiate_structure[neighbor] = extract_function_classes(neighbor)\n",
    "            # get_skeleton(raw_code, keep_constant = False, keep_indent=False, total_lines =15, prefix_lines=5,suffix_lines=5)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    with get_openai_callback() as callback:      \n",
    "        filtered_list = select_list_chain.invoke({\"problem_description\": problem_description, \"file_structure\": candiate_structure})\n",
    "        print(callback)\n",
    "    filtered_list = json.loads(filtered_list.additional_kwargs['function_call']['arguments'])\n",
    "    filtered_list = filtered_list['suspicious_files']\n",
    "    \n",
    "    filtered_candidate_structure = {}\n",
    "    for file in filtered_list:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_code = f.read()\n",
    "            filtered_candidate_structure[file]=get_skeleton(raw_code, keep_constant = False, keep_indent=True, total_lines =15, prefix_lines=5,suffix_lines=5)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    with get_openai_callback() as callback:  \n",
    "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": filtered_candidate_structure})\n",
    "        print(callback)\n",
    "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
    "        \n",
    "    return answer['suspicious_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_problem_description(inputs):\n",
    "    problem_description = inputs['problem_description']\n",
    "    return problem_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['django/django/db/migrations/serializer.py']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors_by_relation(graph, \"module_serializer\",  'path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_suspicious_files_using_clfn(inputs):\n",
    "    \"\"\"\n",
    "    Given a graph and a file, find the most suspicious files related to the given file.\n",
    "    \"\"\"\n",
    "    \n",
    "    problem_description = inputs['problem_description']\n",
    "    graph = inputs['graph']\n",
    "    class_function_name = inputs['class_function_name']\n",
    "    print(class_function_name)\n",
    "\n",
    "    if \".\" in class_function_name:\n",
    "        class_function_name = class_function_name.split(\".\")[0]\n",
    "    suspicious_files = []\n",
    "    for neighbor in neighbors_by_relation(graph, \"class_\"+class_function_name,  'class_path'):\n",
    "        if \"test\" in neighbor:\n",
    "            continue\n",
    "        suspicious_files.append(neighbor)\n",
    "    print(\"class\", suspicious_files)\n",
    "    # filtered = select_list_class_chain.invoke({\"problem_description\": problem_description, \"file_list\": suspicious_files})\n",
    "    # filtered = json.loads(filtered.additional_kwargs['function_call']['arguments'])\n",
    "    selected_file = suspicious_files #filtered['suspicious_files']\n",
    "    # print(selected_file)\n",
    "    \n",
    "    filtered_candidate_structure = {}\n",
    "    for file in selected_file:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_code = f.read()\n",
    "            filtered_candidate_structure[file]=get_skeleton(raw_code, keep_constant = False, keep_indent=True, total_lines =15, prefix_lines=5,suffix_lines=5)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    with get_openai_callback() as callback:  \n",
    "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": filtered_candidate_structure})\n",
    "        print(callback)\n",
    "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
    "        \n",
    "    return answer['suspicious_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_suspicious_files_using_file_structure(inputs):\n",
    "    \"\"\"\n",
    "    Given a graph and a file, find the most suspicious files related to the given file.\n",
    "    \"\"\"\n",
    "    name = inputs['name']\n",
    "    problem_description = inputs['problem_description']\n",
    "    \n",
    "    file_structure = get_file_structure(name)\n",
    "    \n",
    "    directories = file_structure.keys()\n",
    "    with get_openai_callback() as callback:  \n",
    "        filtered = select_directory_chain.invoke({\"problem_description\": problem_description, \"directory_list\": directories})\n",
    "        print(callback)\n",
    "    filtered = json.loads(filtered.additional_kwargs['function_call']['arguments'])\n",
    "    selected_directory = filtered['suspicious_directory']\n",
    "    if selected_directory in file_structure:\n",
    "        suspicious_files = file_structure[selected_directory]\n",
    "    elif name + \"/\" + selected_directory in file_structure:\n",
    "        suspicious_files = file_structure[name + \"/\" + selected_directory]\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    print(len(suspicious_files))\n",
    "    candiate_structure = {}\n",
    "    for file in suspicious_files:\n",
    "        try:\n",
    "            candiate_structure[file] = extract_function_classes(file)\n",
    "            # get_skeleton(raw_code, keep_constant = False, keep_indent=False, total_lines =15, prefix_lines=5,suffix_lines=5)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    with get_openai_callback() as callback:    \n",
    "        filtered_list = select_list_chain.invoke({\"problem_description\": problem_description, \"file_structure\": candiate_structure})\n",
    "        print(callback)\n",
    "    filtered_list = json.loads(filtered_list.additional_kwargs['function_call']['arguments'])\n",
    "    filtered_list = filtered_list['suspicious_files']\n",
    "    \n",
    "    \n",
    "    filtered_candidate_structure = {}\n",
    "    for file in filtered_list:\n",
    "        try:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_code = f.read()\n",
    "            filtered_candidate_structure[file]=get_skeleton(raw_code, keep_constant = False, keep_indent=True, total_lines =15, prefix_lines=5,suffix_lines=5)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    with get_openai_callback() as callback:\n",
    "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": filtered_candidate_structure})\n",
    "        print(callback)\n",
    "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
    "    print(\"file\", answer['suspicious_files'])\n",
    "        \n",
    "    return answer['suspicious_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "import json\n",
    "\n",
    "def embedding_retriever(inputs):\n",
    "    \n",
    "    problem_description = inputs['problem_description']\n",
    "    name = inputs['name']\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    vector_store = FAISS.load_local(\n",
    "    f\"{name}_faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    "    )\n",
    "    results = vector_store.similarity_search(problem_description, k=5,)\n",
    "    bm_retriever = BM25Retriever.from_documents(results,k=3)\n",
    "    results = bm_retriever.invoke(problem_description)\n",
    "    structure = {}\n",
    "    for result in results:\n",
    "        structure[result.metadata['filename']] = result.page_content\n",
    "        \n",
    "    with get_openai_callback() as callback:\n",
    "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": structure})\n",
    "        print(callback)\n",
    "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
    "        \n",
    "    return answer['suspicious_files']\n",
    "\n",
    "    # return results\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 1797\n",
      "\tPrompt Tokens: 1527\n",
      "\t\tPrompt Tokens Cached: 1408\n",
      "\tCompletion Tokens: 270\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00028544999999999997\n",
      "[{'file': 'django/tests/staticfiles_tests/storage.py', 'reason': \"The issue relates to handling the 'If-Modified-Since' header, which is likely to involve storage mechanisms for static files. The classes in this file, particularly those extending 'storage.Storage', may have methods that interact with headers or caching mechanisms, making it a candidate for where the empty string handling might be incorrectly implemented.\"}, {'file': 'django/tests/view_tests/views.py', 'reason': \"This file contains various view functions that could potentially handle HTTP requests and responses. Since the issue is about the 'If-Modified-Since' header, which is part of HTTP requests, the view functions may be responsible for processing this header. If any of these views are designed to handle conditional requests, they might be where the empty string handling is failing.\"}, {'file': 'django/django/core/mail/message.py', 'reason': \"Although primarily focused on email messages, this file contains functions and classes that deal with headers. The 'forbid_multi_line_headers' function suggests that there is some handling of header values, which could be relevant to the 'If-Modified-Since' header. If there are any interactions with HTTP headers in this context, it could be a source of the issue.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_retriever({\"problem_description\":problem_description, \"name\": name}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_reasoning(inputs):\n",
    "    candidates = []\n",
    "    if inputs['get_suspicious_files']:\n",
    "        candidates.extend(inputs['get_suspicious_files'])\n",
    "    if inputs['get_suspicious_files_using_clfn']:\n",
    "        candidates.extend(inputs['get_suspicious_files_using_clfn'])\n",
    "    if inputs['get_suspicious_files_using_file_structure']:\n",
    "        candidates.extend(inputs['get_suspicious_files_using_file_structure'])\n",
    "    if inputs['embedding_retriever']:   \n",
    "        candidates.extend(inputs['embedding_retriever'])\n",
    "    \n",
    "    for c in candidates:\n",
    "        print(c)\n",
    "    problem_description = inputs['problem_description']\n",
    "    with get_openai_callback() as callback:  \n",
    "        result = deep_reasoning_chain.invoke({\"problem_description\": problem_description, \"candidates\": candidates})\n",
    "        print(callback)\n",
    "    result = result.content\n",
    "    \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableSequence\n",
    "\n",
    "start_run = RunnableLambda(start)\n",
    "pass_problem_description_run = RunnableLambda(pass_problem_description)\n",
    "get_suspicious_files_run = RunnableLambda(get_most_suspicious_files)\n",
    "get_suspicious_files_using_clfn_run = RunnableLambda(get_most_suspicious_files_using_clfn)\n",
    "get_suspicious_files_using_file_structure_run = RunnableLambda(get_most_suspicious_files_using_file_structure)\n",
    "embedding_retriever_run = RunnableLambda(embedding_retriever)\n",
    "final_reasoning_rub = RunnableLambda(final_reasoning)\n",
    "\n",
    "parallel_run = RunnableParallel(\n",
    "    {\n",
    "        \"get_suspicious_files\": get_suspicious_files_run,\n",
    "        \"get_suspicious_files_using_clfn\": get_suspicious_files_using_clfn_run,\n",
    "        \"get_suspicious_files_using_file_structure\": get_suspicious_files_using_file_structure_run,\n",
    "        \"embedding_retriever\": embedding_retriever_run,\n",
    "        'problem_description': pass_problem_description_run\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "django\n",
      "DiGraph with 28324 nodes and 41362 edges\n",
      "Checked out to 4fd3044ca0135da903a70dfb66992293f529ecf1\n",
      "DiGraph with 27538 nodes and 40138 edges\n",
      "Tokens Used: 438\n",
      "\tPrompt Tokens: 408\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 30\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0013199999999999998\n",
      "__call__\n",
      "class ['django/django/contrib/contenttypes/fields.py', 'django/django/contrib/gis/geos/libgeos.py', 'django/django/contrib/gis/geos/prototypes/threadsafe.py', 'django/django/contrib/gis/management/commands/ogrinspect.py', 'django/django/contrib/postgres/fields/array.py', 'django/django/contrib/postgres/fields/hstore.py', 'django/django/contrib/postgres/validators.py', 'django/django/contrib/staticfiles/handlers.py', 'django/django/contrib/syndication/views.py', 'django/django/core/handlers/wsgi.py', 'django/django/core/serializers/__init__.py', 'django/django/core/validators.py', 'django/django/db/models/fields/json.py', 'django/django/db/models/fields/related_descriptors.py', 'django/django/db/utils.py', 'django/django/utils/deprecation.py', 'django/django/utils/html.py']\n",
      "Tokens Used: 400\n",
      "\tPrompt Tokens: 374\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 26\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $7.17e-05\n",
      "Tokens Used: 1092\n",
      "\tPrompt Tokens: 1055\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 37\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00018044999999999997\n",
      "Tokens Used: 2261\n",
      "\tPrompt Tokens: 2237\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 24\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00034995\n",
      "9\n",
      "Tokens Used: 10975\n",
      "\tPrompt Tokens: 10816\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 159\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0017178\n",
      "Tokens Used: 2402\n",
      "\tPrompt Tokens: 2359\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 43\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00037964999999999993\n",
      "Tokens Used: 1268\n",
      "\tPrompt Tokens: 1095\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 173\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00026805\n",
      "Tokens Used: 5914\n",
      "\tPrompt Tokens: 5732\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 182\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0009689999999999999\n",
      "Tokens Used: 6455\n",
      "\tPrompt Tokens: 6235\n",
      "\t\tPrompt Tokens Cached: 0\n",
      "\tCompletion Tokens: 220\n",
      "\t\tReasoning Tokens: 0\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00106725\n",
      "file [{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class is defined here, which is responsible for handling URL validation. The issue specifically mentions that the clean method of URLField throws a ValueError instead of a ValidationError, indicating that the problem likely lies within the implementation of this class.'}, {'file': 'django/django/forms/forms.py', 'reason': \"The BaseForm class and its methods, including clean, are defined here. Since the URLField is used within forms, any issues with the form's validation process could also contribute to the error being thrown. This file may contain logic that interacts with the URLField's clean method.\"}, {'file': 'django/django/core/validators.py', 'reason': 'The issue involves URL validation, which is typically handled by validators. The URLValidator class is likely defined in this file, and if it is incorrectly implemented or not handling certain cases properly, it could lead to the ValueError being raised instead of the expected ValidationError.'}]\n",
      "{'file': 'django/django/core/validators.py', 'reason': \"The URLValidator class is responsible for validating URLs, including IPv4 and IPv6 formats. The issue indicates that a ValueError is thrown instead of a ValidationError when an invalid URL is provided. This suggests that the URLValidator's implementation may not be handling certain invalid inputs correctly, particularly those that should trigger a ValidationError.\"}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class inherits from CharField and uses the URLValidator for validation. The clean method in the Field class (from which URLField inherits) is responsible for calling the validators. If the URLField is not correctly handling the exceptions raised by the URLValidator, it could lead to the observed ValueError instead of the expected ValidationError.'}\n",
      "{'file': 'django/django/core/validators.py', 'reason': \"The issue involves the URLField's clean method throwing a ValueError instead of a ValidationError. The URLValidator class in this file is responsible for validating URLs, including handling invalid formats. Since the error occurs during URL validation, this file is directly related to the issue.\"}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class is defined in this file, and its clean method is invoked in the issue description. This method is responsible for running validators, including the URLValidator. Therefore, any issues with how the clean method processes input or interacts with validators could lead to the described error.'}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class is defined here, which is responsible for handling URL validation. The issue specifically mentions that the clean method of URLField throws a ValueError instead of a ValidationError, indicating that the problem likely lies within the implementation of this class.'}\n",
      "{'file': 'django/django/forms/forms.py', 'reason': \"The BaseForm class and its methods, including clean, are defined here. Since the URLField is used within forms, any issues with the form's validation process could also contribute to the error being thrown. This file may contain logic that interacts with the URLField's clean method.\"}\n",
      "{'file': 'django/django/core/validators.py', 'reason': 'The issue involves URL validation, which is typically handled by validators. The URLValidator class is likely defined in this file, and if it is incorrectly implemented or not handling certain cases properly, it could lead to the ValueError being raised instead of the expected ValidationError.'}\n",
      "{'file': 'django/django/utils/ipv6.py', 'reason': 'The `clean_ipv6_address` function is responsible for cleaning and validating IPv6 addresses. Since the issue involves a ValueError being raised instead of a ValidationError when an invalid IPv6 URL is provided, this file is likely to contain the logic that should handle such validation and may not be correctly raising the expected ValidationError.'}\n",
      "{'file': 'django/django/core/validators.py', 'reason': \"The `validate_ipv6_address` function is likely involved in the validation process for IPv6 addresses. If this function is not correctly implemented or not being called when the URLField's `clean` method is executed, it could lead to the observed ValueError instead of the expected ValidationError.\"}\n",
      "Tokens Used: 2440\n",
      "\tPrompt Tokens: 1056\n",
      "\t\tPrompt Tokens Cached: 128\n",
      "\tCompletion Tokens: 1384\n",
      "\t\tReasoning Tokens: 881\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "full_flow = start_run | parallel_run | final_reasoning_rub\n",
    "args = {\"problem_description\": problem_description, \"name\": name, \"graph\": graph, \"commit_id\": commit_id}\n",
    "result = full_flow.invoke(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Answer:**\n",
      "\n",
      "Let's evaluate each candidate file's relevance to the issue and assign confidence scores:\n",
      "\n",
      "1. **`django/django/contrib/admin/options.py`**  \n",
      "   **Confidence Score: 90/100**  \n",
      "   **Reasoning:**  \n",
      "   This file controls the admin's form and formset handling, including validation and re-rendering after errors. The issue arises because the admin reinitializes inline formsets with **submitted data** as the new \"initial\" values after a validation error. This causes the hidden input (`initial-relatedmodel_set-0-plop`) to reflect the submitted data (e.g., `\"test\"`) instead of the model's original default (`list`). On resubmission, the form compares the new submission (empty) against the updated initial (`\"test\"`), bypassing validation. The user’s fix (`show_hidden_initial=False`) avoids this by forcing the original default to persist. The logic for rebuilding formsets with incorrect initial data likely resides here.\n",
      "\n",
      "2. **`django/django/forms/models.py`**  \n",
      "   **Confidence Score: 70/100**  \n",
      "   **Reasoning:**  \n",
      "   The `ModelForm` class (in this file) determines how initial values are propagated to form fields. If the field’s `show_hidden_initial` is enabled (default for callable defaults like `list`), the hidden input tracks the initial value. The `get_initial_for_field` method or form initialization might inadvertently use stale initial data after validation errors. However, the primary issue lies in how the admin *reinitializes* formsets (in `options.py`), not the `ModelForm` itself.\n",
      "\n",
      "3. **`django/django/forms/forms.py`**  \n",
      "   **Confidence Score: 40/100**  \n",
      "   **Reasoning:**  \n",
      "   The `Form` class’s `has_changed()` method compares `data` against `initial`. If the admin incorrectly sets `initial` to the submitted data (as in `options.py`), this method would falsely flag the form as unchanged. While critical to the comparison logic, the root cause is upstream in how `initial` is set by the admin.\n",
      "\n",
      "4. **`django/django/contrib/admin/forms.py`**  \n",
      "   **Confidence Score: 30/100**  \n",
      "   **Reasoning:**  \n",
      "   Admin-specific forms might influence validation, but the issue is not unique to admin forms. The problem occurs because the admin reinitializes formsets with invalid data as `initial`, which is handled in `options.py`, not form classes here.\n",
      "\n",
      "5. **`django/django/forms/widgets.py`**  \n",
      "   **Confidence Score: 20/100**  \n",
      "   **Reasoning:**  \n",
      "   The `HiddenInput` widget renders the hidden initial value, but the value itself is determined by the admin’s form initialization. The widget isn’t the root cause but reflects incorrect data passed from `options.py`.\n",
      "\n",
      "6. **`admin.py` (user’s code)**  \n",
      "   **Confidence Score: 15/100**  \n",
      "   **Reasoning:**  \n",
      "   While the `RelatedModelForm` triggers the error, the issue stems from Django’s handling of initial values, not the form’s custom logic. The fix (`show_hidden_initial=False`) is a workaround, not a solution.\n",
      "\n",
      "7. **`models.py` (user’s code)**  \n",
      "   **Confidence Score: 10/100**  \n",
      "   **Reasoning:**  \n",
      "   The `ArrayField`’s `default=list` is correct. The problem is not the model’s default but how the admin propagates initial values across submissions.\n",
      "\n",
      "8. **`django/django/contrib/admin/utils.py`**  \n",
      "   **Confidence Score: 10/100**  \n",
      "   **Reasoning:**  \n",
      "   Utility functions might support formset creation, but no direct evidence ties them to the reinitialization bug.\n",
      "\n",
      "9. **Other test files (e.g., `tests/model_forms/tests.py`)**  \n",
      "   **Confidence Score: 5/100**  \n",
      "   **Reasoning:**  \n",
      "   Test files validate existing behavior but aren’t responsible for runtime bugs.\n",
      "\n",
      "---\n",
      "\n",
      "**Conclusion:**  \n",
      "The **`options.py`** file is most likely responsible. The admin incorrectly reinitializes formsets with submitted data as the new `initial` values after validation errors, causing the hidden input to track invalid data and bypass subsequent validation. Fixing this would require adjusting how the admin rebuilds formsets during validation failures.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50 boundfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class is defined here, which is responsible for handling URL inputs. The issue specifically mentions that the clean method of URLField throws a ValueError instead of a ValidationError. Since the clean method is part of the URLField class, this file is directly related to the problem.'}\n",
      "{'file': 'django/django/core/validators.py', 'reason': \"The URLValidator class is defined in this file, which is likely used by the URLField to validate the URL input. The issue indicates that the validation process is resulting in a ValueError, which suggests that the URLValidator's implementation may be involved in the error handling process. Therefore, this file is also suspicious.\"}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The issue involves the `URLField` class, which is defined in this file. The `clean` method of `URLField` is responsible for validating the input URL, and the error message indicates that a `ValueError` is being raised instead of a `ValidationError`. This suggests that the validation logic within the `clean` method or the `run_validators` method may not be handling invalid URLs correctly.'}\n",
      "{'file': 'django/django/core/validators.py', 'reason': 'The `URLValidator` class is defined in this file and is used by the `URLField` for validating URLs. The error message indicates that the URL validation is failing, leading to a `ValueError`. This suggests that the regex patterns or validation logic in the `URLValidator` may be causing the incorrect exception to be raised.'}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The `Field` class, which is a parent class of `URLField`, contains the `clean` method that is likely being overridden by `URLField`. If there are issues with how errors are raised in the `clean` method, it could lead to the observed behavior of raising a `ValueError` instead of a `ValidationError`.'}\n",
      "{'file': 'django/django/forms/fields.py', 'reason': 'The URLField class is defined in this file, and it is responsible for validating URLs. The issue specifically mentions that a ValueError is thrown instead of a ValidationError when cleaning a URL, which indicates that the logic in the clean method or the validators used in URLField may not be handling invalid URLs correctly.'}\n",
      "{'file': 'django/django/forms/forms.py', 'reason': \"This file contains the BaseForm and Form classes, which manage the overall form validation process. The issue arises during the cleaning of the URLField, which is part of a form. If the form's validation process is not correctly set up to catch and handle errors from fields like URLField, it could lead to the observed ValueError instead of the expected ValidationError.\"}\n",
      "{'file': 'django/django/core/validators.py', 'reason': 'The traceback indicates that the URL validation is being handled by the validators defined in this file. Since the URLField uses the URLValidator, any issues with how this validator processes invalid URLs could directly lead to the ValueError being raised instead of a ValidationError.'}\n",
      "{'file': 'django/django/utils/ipv6.py', 'reason': \"The function 'clean_ipv6_address' is designed to clean and validate IPv6 addresses. Since the issue involves a ValueError being raised instead of a ValidationError when an invalid IPv6 URL is provided, this file is likely responsible for the validation logic that should be invoked during the cleaning process.\"}\n",
      "{'file': 'django/django/core/validators.py', 'reason': \"The 'validate_ipv6_address' function is likely part of the validation process for IPv6 addresses. If this function is not correctly integrated or called during the URLField's cleaning process, it could lead to the observed ValueError instead of the expected ValidationError.\"}\n"
     ]
    }
   ],
   "source": [
    "# for r in result:\n",
    "#     print(r)\n",
    "#i =27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
