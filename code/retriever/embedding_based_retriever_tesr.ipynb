{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "import pickle\n",
        "import networkx as nx\n",
        "import ast\n",
        "import re\n",
        "import json\n",
        "import chromadb\n",
        "import cProfile\n",
        "import pstats\n",
        "import weave\n",
        "\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
        "from langchain.agents import tool\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.callbacks import get_openai_callback\n",
        "\n",
        "from commit import update\n",
        "from utils.utils import serialize_dict_to_json, deserialize_json_to_dict\n",
        "from utils.chunk import SimpleFixedLengthChunker\n",
        "from utils.compress import get_skeleton\n",
        "\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']\n",
        "\n",
        "sflc = SimpleFixedLengthChunker()\n",
        "\n",
        "dataset = load_dataset(\"lahirum/SWE_Experimental\", split=\"train\")\n",
        "# filter = [0, 1, 2, 3, 4,5, 6, 7, 8, 9]\n",
        "# dataset = dataset.select(filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4fa7cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_graph(pickle_path):\n",
        "    \"\"\"Loads a NetworkX DiGraph from a pickle file.\"\"\"\n",
        "    with open(pickle_path, \"rb\") as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import prompts\n",
        "import schema\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "# from langchain_deepseek import ChatDeepSeek\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    #max_retries=2,\n",
        ")\n",
        "\n",
        "# llm_deepseek = ChatDeepSeek(\n",
        "#     model=\"deepseek-reasoner\",\n",
        "#     temperature=0,\n",
        "#     max_tokens=None,\n",
        "#     timeout=None,\n",
        "#     max_retries=2,\n",
        "#     # other params...\n",
        "# )\n",
        "\n",
        "llm_large = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0,\n",
        "    max_retries=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2775380/3732032862.py:4: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
            "  functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n"
          ]
        }
      ],
      "source": [
        "parser = JsonOutputFunctionsParser()\n",
        "\n",
        "model_extract = llm_large.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousComponentOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "extract_chain = prompts.prompt_extract | model_extract\n",
        "\n",
        "model_select = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.FileSuspicionOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_chain = prompts.file_path_filter_prompt | model_select\n",
        "\n",
        "model_filter_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "filter_list_chain = prompts.get_suspicious_file_list_from_list_of_files_prompt | model_filter_list \n",
        "\n",
        "model_select_list = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFilesOutputList)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_list_chain = prompts.suspicious_files_filter_list_usingclfn_prompt | model_select_list\n",
        "\n",
        "model_select_with_reason = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousFileReasoningOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_with_reason_chain = prompts.suspicious_files_with_reason_prompt | model_select_with_reason\n",
        "\n",
        "model_select_directory = llm.bind(\n",
        "    functions=[convert_pydantic_to_openai_function(schema.SuspiciousDirectoryOutput)],\n",
        "    function_call=\"auto\",\n",
        ")\n",
        "select_directory_chain = prompts.suspicious_directory_prompt | model_select_directory\n",
        "\n",
        "generate_multiple_descriptions = prompts.prompt_embedding_retriver | llm\n",
        "\n",
        "# deep_reasoning_chain = prompts.deep_reasoning_prompt | llm_deepseek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def start(inputs):\n",
        "#     problem_description = inputs['problem_description']\n",
        "#     name = inputs['name']\n",
        "#     graph = inputs['graph'] \n",
        "#     commit_id = inputs['commit_id']\n",
        "#     graph = inputs['graph'] \n",
        "\n",
        "#     update(name, commit_id)\n",
        "\n",
        "#     with get_openai_callback() as callback:\n",
        "#         result = extract_chain.invoke({\"problem_description\": problem_description})\n",
        "#         print(callback)\n",
        "\n",
        "#     result = result.additional_kwargs['function_call']['arguments']\n",
        "#     result = json.loads(result)\n",
        "#     result['name'] = name\n",
        "#     result['problem_description'] = problem_description\n",
        "#     result['graph'] = graph\n",
        "    \n",
        "#     return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def embedding_retriever(inputs):\n",
        "    problem_description = inputs['problem_description']\n",
        "    name = inputs['name']\n",
        "\n",
        "    with get_openai_callback() as callback:\n",
        "        multiple_descriptions = generate_multiple_descriptions.invoke({\"problem_description\": inputs[\"problem_description\"]})\n",
        "        print(callback)\n",
        "\n",
        "    problem_description = \\\n",
        "        f\"\"\"## **Original GitHub issue description**:\\n\\n{inputs[\"problem_description\"]}\\n\\n\\n## **Generated descriptions**:\\n\\n{multiple_descriptions.content}\"\"\"\n",
        "    \n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "    chroma_client = chromadb.PersistentClient(f\"chroma_db\")\n",
        "    collection = chroma_client.get_collection(name=f\"{name}_chroma_index\")\n",
        "\n",
        "    vector_store = Chroma(\n",
        "        client=chroma_client,\n",
        "        collection_name=f\"{name}_chroma_index\",\n",
        "        embedding_function=embeddings,\n",
        "    )\n",
        "\n",
        "    results = vector_store.similarity_search(problem_description, k=10,)\n",
        "    # results = vector_store.max_marginal_relevance_search(problem_description, k=10, lambda_mult=0.5)\n",
        "\n",
        "    file = deserialize_json_to_dict(\"django_file_ids.json\")\n",
        "    structure = {}\n",
        "    for result in results:\n",
        "        file_ids = file[result.metadata[\"filename\"]].split(\":\")\n",
        "        chunk_docs_of_file = vector_store.get_by_ids(file_ids)\n",
        "        structure[result.metadata['filename']] = get_skeleton(sflc.dechunk_docs(chunk_docs_of_file))\n",
        "        \n",
        "    with get_openai_callback() as callback:\n",
        "        answer = select_with_reason_chain.invoke({\"problem_description\": problem_description, \"file_structure\": structure})\n",
        "        print(callback)\n",
        "    answer = json.loads(answer.additional_kwargs['function_call']['arguments'])\n",
        "\n",
        "    return answer['suspicious_files']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: eshangj.\n",
            "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/eshangj-debug-ai/debug-ai-embedding-based-retriever/weave\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m\u001b[1mweave\u001b[0m: üç© https://wandb.ai/eshangj-debug-ai/debug-ai-embedding-based-retriever/r/call/019731eb-7ea6-7af1-a2e0-bcb0c0060112\n"
          ]
        }
      ],
      "source": [
        "weave.init('debug.ai-embedding-based-retriever')\n",
        "\n",
        "def custom_attribute_name(call):\n",
        "    instance_id = call.attributes[\"instance_id\"]\n",
        "    return f\"{instance_id}\"\n",
        "\n",
        "@weave.op(call_display_name=custom_attribute_name)\n",
        "def run(ips):\n",
        "    update(ips['name'], ips['commit_id'])\n",
        "    ans = embedding_retriever(ips)\n",
        "    temp_dict = {}\n",
        "    for level, a in enumerate(ans):\n",
        "        temp_dict[f\"identified_file_level_{level}\"] = a['file']\n",
        "\n",
        "    return temp_dict\n",
        "\n",
        "for i in range(0, 10):\n",
        "    ips = {}\n",
        "\n",
        "    ips[\"instance_id\"] = dataset[i]['instance_id']\n",
        "    ips[\"commit_id\"] = dataset[i]['base_commit']\n",
        "    ips[\"name\"]= dataset[i]['instance_id'].split(\"__\")[0]\n",
        "    ips[\"problem_description\"] = dataset[i]['problem_statement']\n",
        "    ips[\"graph\"] = load_graph(f\"graph_{ips['name']}.pkl\")\n",
        "    ips[\"erroneous_file\"] = dataset[i][\"erroneous_file\"]\n",
        "\n",
        "    with weave.attributes({\"instance_id\": f\"{ips['instance_id']}\"}):\n",
        "        run(ips)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
